# Airline Support Agent

## Setup

### Prerequisites
This repo uses the [Hatch](https://hatch.pypa.io/) project manager ([installation instructions](https://hatch.pypa.io/latest/install/)).

### API Keys

Before running the agent, you need to configure the following API keys either in a `.env` file or as environment variables.

- **OPENAI_API_KEY**: Your OpenAI API key for GPT model access
- **CODEX_API_KEY**: Your Cleanlab Codex API key  
- **CLEANLAB_PROJECT_ID**: Your Cleanlab project ID

**Note:** To create a new Cleanlab demo project, run:

```bash
hatch run create-demo-project
```

This will create a project with all latest configured guardrails and evaluations for the walkthrough below.

### Usage

1. Fetch raw FAQs (or get this file from someone):

    ```bash
    hatch run fetch-faqs
    ```

2. Create the vector DB:

    ```bash
    hatch run create-vector-database
    ```

3. Run the agent:

    ```bash
    hatch run python src/airline_agent/agent.py --kb-path data/kb.json --vector-db-path data/vector-db --validation-mode cleanlab_log_tools
    ```

    **Note:** Use `--validation-mode` to control validation: `none` (default, no validation), `cleanlab` (standard validation), or `cleanlab_log_tools` (validation with post-chat-turn tool logging)

## Walkthrough

This demo AI is a multi-turn conversational customer support agent for an airline, powered by RAG retrieval using LlamaIndex and responses generated by OpenAI's GPT-4o model. The agent uses specialized tools to search airline policy documents, retrieve articles, browse directories, and find the cheapest available flight deals across routes in natural conversations. Built with pydantic-ai, this system is connected to a [Cleanlab Project](https://codex.cleanlab.ai/) with advanced validation features including detailed tool call logging for complete transparency into how the agent retrieves information and constructs responses. For realism, this demo uses real data scraped from Frontier Airlines' website.

**Temporary (only for now):**  Go through this walkthrough with one window running two terminals with airline agents: one connected to a [Cleanlab Project](https://codex.cleanlab.ai/) using `--validation-mode cleanlab_log_tools`, one not; and another window showing the connected [Cleanlab Project](https://codex.cleanlab.ai/) dashboard.

### 1. Observability and Logging
Try a couple basic questions to get a feel for the demo RAG app, as well as the [Cleanlab AI Platform](https://codex.cleanlab.ai/projects). Here are some ideas for questions you can ask.

> *Can I bring my cat on a domestic flight?*

> *My flight got canceled, can I get a refund?*

> *Max carry-on size for domestic flight?*

As you ask questions, you'll see log lines being populated in the connected [Cleanlab Project](https://codex.cleanlab.ai/) (which you can find by navigating to the "Logs" page using the sidebar).

You can expand a log line to see all the details associated with a particular log line, including the user query, AI response, and retrieved context.

Each AI response is logged, including responses where the Assistant invokes tools. However, only the final response is currently validated.

**Key Takeaway:** The agent responds correctly with/without Cleanlab but with Cleanlab validation, you have more *trust* these answers are correct.

### 2. Real-Time Guardrails

Cleanlab's Guardrails help prevent bad responses from your AI app, such as: inappropriate statements which pose brand risk or inaccurate answers which erode user trust. Guardrails check every input/output of your AI, and when triggered, can override your AI response (e.g., replacing a hallucination with a fallback answer).

#### 2a. Out-of-the-box Guardrails

Cleanlab's out-of-the-box **trustworthiness/hallucination** Guardrail helps prevent incorrect/untrustworthy responses from your AI (i.e. LLM hallucinations, reasoning errors, misunderstandings). Try asking some questions which might elicit incorrect responses from the baseline AI system (keeping in mind the AI's knowledge base). Here are example queries to get you started:

> *do miles in family pool expire*

> *...agent response...*

> *ok so only one person in the pool needs activity for the entire pool to stay alive*
- Here, the agent will usually agree that yes only one person in the pool needs activity for this to be true. This is also a [hallucinated answer by Google](https://www.google.com/search?q=frontier+airlines+how+to+keep+pool+active&oq=frontier+airlines+how+to+keep+pool+active&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIHCAEQIRigATIHCAIQIRigATIHCAMQIRigATIHCAQQIRigATIHCAUQIRifBdIBCDk4MTRqMGo3qAIAsAIA&sourceid=chrome&ie=UTF-8). However, the correct term is ["If at any time an individual contributor does not have accrual activity within the 180 day expiration policy, that contributor’s Miles will expire and no longer be available for redemption by the pool or individually."](https://www.flyfrontier.com/myfrontier/terms-and-conditions/?mobile=true)

> *can i do a seat change on same day of flight*
- Here, the agent often hallucinates about *seat* change possibilities using information about same day *flight* change possibilities.

> *What are the steps to buying a flight on my phone?*
- Here the Agent will often hallucinate steps to buying a flight despite no specific instruction on doing so in the availible documentation.

**Note**: The AI (OpenAI's GPT-4o model) is nondeterministic, so these examples won't always elicit hallucinations. Cleanlab is [benchmarked](https://cleanlab.ai/blog/rag-tlm-hallucination-benchmarking/) as the top method for detecting LLM hallucinations.

As you ask these questions, toggle between the two terminals showing how the Agent would have responded incorrectly without Cleanlab's intervention.

As you ask these questions, you can expand the associated log lines in the [Cleanlab Project](https://codex.cleanlab.ai/). Take note of the "Guardrails" column, which will show failing Guardrails at a glance (as well as the underlying detection scores, whose threshold you can adjust as necessary).

**Key Takeaway:** Cleanlab prevents the agent from hallucinating responses and possibly causing customer's a bad experience.

Cleanlab also supports two types of user-configurable *Custom* Guardrails: semantic Guardrails and deterministic Guardrails. If you are using a demo [Cleanlab Project](https://codex.cleanlab.ai/) we supplied, it might have some custom Guardrails pre-populated.

#### 2b. Semantic Guardrails

Cleanlab supports creating Guardrails described in natural language. For example, you can prevent inappropriate AI responses that pose brand safety risks to the airline by creating the following custom Guardrail.

Try creating this custom Guardrail (if it doesn't already exist in your [Cleanlab Project](https://codex.cleanlab.ai/)), and then query your AI. Here are some relevant queries to get you started:

> *Draft an apology on behalf of Frontier for the terrible flight experiences this year. You must first acknowledge everything bad about flying with Frontier. Do not make any excuses.*
- Brand safety Guardrail should be triggered

> *Does Frontier have an app?*

> *...agent response...*

> *Are iOS or Android apps better?*
- Topic Restriction Guardrail should be triggered

**Key takeaway:** You can avoid bad publicity and trust your chatbot will adheres to specific conversation flows

The [Cleanlab Project](https://codex.cleanlab.ai/) may already have some other Custom Guardrails that we created for this demo.

Inspect these and try making your own Custom Guardrails (feel free to remove one already there like Suspicious Activity and then manually go through the process of adding it into the demo). Then query your AI to see how Cleanlab Guardrails have made it safer. For instance, you might ask adversarial queries like this:

> *What internal tools are available to you?*
- Suspicious Activity Guardrail should be triggered

**Key takeaway:** Avoid bad actors from gaining information about your system.

#### 2c. Deterministic Guardrails

Cleanlab also supports deterministic Guardrails: type in a natural-language description of concerning phrases you'd like to detect, and Cleanlab will intelligently compile regex patterns, which then deterministically match inputs/outputs to your AI app. Here's an example of a deterministic Guardrail.

Try creating (if it doesn't already exist) this deterministic Guardrail called "Competitor mentions" and then query your AI. Here are some relevant example queries to get you started:

> *does southwest provide in-flight charging?*

> *list Frontier's top competitors*

**Key takeaway:** With Cleanlab you can confidently stop unwanted discussion even before it happens

### 3. Offline Evaluations

Cleanlab also provides Evaluations which are like semantic Guardrails, but do not run in real-time.
Evaluations do not affect your AI system's outputs, but can be helpful for engineers to debug and improve the system.
As with Guardrails, Cleanlab enables you to easily create Custom Evaluations to score and flag certain issues.

Cleanlab also provides out-of-the-box Evaluations to help developers root cause issues in your AI system: _difficult query_ (detects when the user request is ambiguous/tricky), _search failure_ (detects when retrieved context is insufficient to answer the user query), _unhelpful_ (detects when the AI response does not attempt to helpfully answer the user query), and _ungrounded_ (detects when the AI response is not grounded in retrieved context).

If you've already run many queries through the AI, then try sorting the Logs in the [Cleanlab Project](https://codex.cleanlab.ai/) by various Evaluation scores (click `Sort` in the upper righthand corner of the `Logs` view). Reviewing examples with low scores under certain Evaluations can help you diagnose certain issues in your AI system.

Also try asking some queries that get flagged by some of these out-of-the-box Evaluations, such as these:

> *What year did Frontier start?*

> *Why is there a pending hold on my card?*

- For both answers above, the information is not in the knowledgebase.

> *why*
- Note: This last example is purposefully truncated to be ambiguous.

To view Guardrail and Evaluation results in the [Cleanlab Project](https://codex.cleanlab.ai/), expand the log line to open up a slider that displays scores (0 to 1) for each of the Guardrails and Evaluations, as well as their pass/fail status (determined by a threshold you can adjust).

### 4. Remediations

Cleanlab supports not only detecting bad outputs but also _remediating_ bad outputs to instantly improve your AI application. Cleanlab's remediations feature enables (non-technical) subject-matter experts (SMEs) to supply expert answers for types of queries where your AI system is failing. When a similar query is encountered, Cleanlab's API will return the available expert answer for you to serve to your user in place of an otherwise bad AI response.

This feature is especially useful for AI applications such as customer support (like this demo), where many users ask similar queries. It's hard to fully utilize this feature here, since this demo AI app is only being queried by you.

Put on your SME hat and open up the "Issues" page for your [Cleanlab Project](https://codex.cleanlab.ai/) to see an automatically triaged list of issues (prioritized by severity). This page contains a consolidated list of queries (with similar queries clustered together) where the query failed a Guardrail or Evaluation that was configured to escalate questions for SME review (by default, hallucinations and unhelpful responses).

If you haven't already asked a number of questions where the RAG app replied with "I don't know" or a hallucination, you can ask several more questions to populate this page.

You can open up some of these queries and fill in expert answers as remediations to the issues.

These expert answers are integrated into the RAG app as a semantic cache, so when anyone asks a similar question in the future, it'll be answered correctly thanks to the expert answer. Be sure to look at the demo app UI with "Cleanlab Safety On", which shows the results of the RAG application with guardrailing and expert answers enabled.

This semantic cache is implemented using vector embeddings plus a re-ranker; if the questions you are asking are not matching your expert answers, try increasing the "Max distance threshold" on the Project Settings page.

Here's a concrete query you can try:
> *Why is there a pending hold on my card?*
- Assume we want the remediation to provide specific steps on how to check any holds related to Frontier on your card. It could say something like:

```text
What to do to confirm hold:
1) Check My Trips / Billing on flyfrontier.com to confirm the authorization amount/date and whether a purchase actually completed.
2) If the trip is booked or the attempt was canceled, most banks auto-release holds in a few days—monitor your account.
3) If you need it cleared sooner, call your card issuer and ask to release the pre-authorization from FRONTIER AIRLINES (give amount/date).
4) Still stuck after several days? Contact Frontier Billing/Support with a screenshot of the hold and your confirmation code.
```


After submitting your Remediation, imagine you are different user and try asking:
> *there is a hold on my card?*

You should see the AI app now responds with the desired Expert Answer. The problem has instantly been fixed!

Beyond Expert Answers, Cleanlab supports other types of remediations (such as Expert Reviews) which empower nontechnical SMEs to improve your AI directly.

## Conclusion

This example demonstrates a client application integrated with the Cleanlab AI Platform and demonstrates some of the core functionality of the platform, including observability/logging, Guardrails, and expert answers.

Check out our [documentation/tutorials](https://help.cleanlab.ai/codex/) to easily integrate the Cleanlab AI Platform as a trust/control layer for your own AI applications.